# 第三章   线性神经网络

## 1.线性回归

### 1.1 线性回归的基本元素

+ 1.线性模型
  + ![image](https://user-images.githubusercontent.com/78517435/226922757-d397173d-8bcf-4df7-9103-182225746ee4.png)
  + 其中w为权重，权重决定了每个特征对我们预测值的影响。b为偏置，即当所有特征都取值为零时，预测值应该为多少
  + 在机器学习领域，我们使用的常是高维数据集，我们将所有特征放到向量X中，将所有权重放到向量w中，那我们可以用向量的点积形式来表达模型
  + ![image](https://user-images.githubusercontent.com/78517435/226924290-ff36e4f9-751a-473a-a407-b5ab0d24f0b4.png)

+ 2.损失函数
  + 损失函数能够量化目标的实际值和预测值之间的差距
  + 回归问题中最常用的损失函数是平方误差函数
![image](https://user-images.githubusercontent.com/78517435/226925709-e629cda6-2f7a-4d4c-8451-37e965737815.png)

+ 3.解析解
  + 回归模型的解可以用一个简单的公式表示出来，这类解叫做解析解，如下：
![image](https://user-images.githubusercontent.com/78517435/226926707-3129fe95-a6ae-4651-834e-86bdf4311a01.png)

 
 + 4.随机梯度下降
   + 这种方法几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。




## 2.线性回归的从零开始实现






## 3.线性回归的简洁实现








## 4.softmax回归







## 5.图像分类数据集










## 6.softmax回归的从零开始实现










## 7.softmax回归的简洁实现
















