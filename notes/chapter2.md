# 第二章  数学知识

## 1.线性代数

### 1.1 向量
1.向量的简单操作：加法，乘法
```python
#利用arange创建向量
x = torch.arange(4)

```
2.向量的长度计算
```python
len(x)
```
3.向量形状
```python
x.shape
```

### 1.2 矩阵
+ 在代码中表示具有两个轴的张量

```python
#创建一个mxn的矩阵
A = torch.arange(20).reshape(4,5)
```
+ 我们可以通过行索引和列索引来访问其中的元素

+ 矩阵的转置(交换矩阵的行和列)
```python
B = A.T
```

### 1.3 张量
+ 张量是描述具有任意数量轴的n维数组的通用方法，向量是一阶张量，矩阵是二阶张量
+ 张量的索引机制与矩阵类似
```python
#创建一个张量
X = torch.arange(24).reshape(2,3,4)
```
### 1.4 张量算法的基本性质
+ 任何按元素的一元操作都不会改变其操作数的形状
+ 给定两相同形状的任意张量，任何按元素的二元运算的结果都将是相同形状的张量
+ 将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘

```python
a = 2
X = torch.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape
```

### 1.5 降维
+ 1.调用求和函数降低维度，我们可以用axis参数指定降维的轴

```python
#输入轴1的维数在输出形状中消失
A_sum_axis1 = A.sum(axis=1)
A_sum_axis1, A_sum_axis1.shape
```

```python
A.sum(axis=[0, 1])  # 结果和A.sum()相同
```

+ 2.调用求平均值的函数也可以指定轴降低维度


+ 3.非降维求和
  + 有时在调用函数来计算总和或均值时保持轴数不变会很有用。

```python
sum_A = A.sum(axis=1, keepdims=True)
sum_A
```
  +由于sum_A在对每行进行求和后仍保持两个轴，我们可以通过广播将A除以sum_A。
```python
A / sum_A
```

### 1.6 点积
+ 给定两个向量，它们的点积是相同位置的按元素乘积的和：

```python
y = torch.ones(4, dtype = torch.float32)
x, y, torch.dot(x, y)
```

### 1.7 矩阵-向量积

![image](https://user-images.githubusercontent.com/78517435/226540900-f385a91b-eec0-4107-8a0c-710a17c7bc79.png)

```python
#使用mv函数表示矩阵-向量积
A.shape, x.shape, torch.mv(A, x)
```

### 1.8 矩阵-矩阵乘法

![image](https://user-images.githubusercontent.com/78517435/226541224-163d4ae0-2b40-40c3-967c-34102122e857.png)

+ 将矩阵-矩阵乘法可以简单地看成执行m次矩阵-向量积，将结果拼接在一起，形成一个nxm的矩阵




### 1.9 范数
+ 向量范数是将向量映射到标量的函数f
  + 第一个性质是如果我们按常数因子缩放向量的所有元素， 其范数也会按相同常数因子的绝对值缩放：
  + 第二个性质是熟悉的三角不等式
  + 第三个性质简单地说范数必须是非负的

+ L2范数我们常常省略下标
+ 深度学习中更经常地使用范数的平方，也会经常遇到范数，它表示为向量元素的绝对值之和
+ 与L2范数相比，L1范数受异常值的影响较小

![image](https://user-images.githubusercontent.com/78517435/226540455-c3db52f9-ca07-4061-a5fc-26a8cdf7bfd3.png)


## 2.微积分
+ 在深度学习中，我们训练模型，并不断更新它们，使它们在数据越来越多的时候表现越来越好，这也就意味着最小化一个损失函数
+ 我们将拟合模型分解为两个关键问题：
  + 优化：用模型拟合观测数据的过程
  + 泛化：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。








## 3.概率












