# 第二章  数学知识

## 1.线性代数

### 1.1 向量
1.向量的简单操作：加法，乘法
```python
#利用arange创建向量
x = torch.arange(4)

```
2.向量的长度计算
```python
len(x)
```
3.向量形状
```python
x.shape
```

### 1.2 矩阵
+ 在代码中表示具有两个轴的张量

```python
#创建一个mxn的矩阵
A = torch.arange(20).reshape(4,5)
```
+ 我们可以通过行索引和列索引来访问其中的元素

+ 矩阵的转置(交换矩阵的行和列)
```python
B = A.T
```

### 1.3 张量
+ 张量是描述具有任意数量轴的n维数组的通用方法，向量是一阶张量，矩阵是二阶张量
+ 张量的索引机制与矩阵类似
```python
#创建一个张量
X = torch.arange(24).reshape(2,3,4)
```
### 1.4 张量算法的基本性质
+ 任何按元素的一元操作都不会改变其操作数的形状
+ 给定两相同形状的任意张量，任何按元素的二元运算的结果都将是相同形状的张量
+ 将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘

```python
a = 2
X = torch.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape
```

### 1.5 降维
+ 1.调用求和函数降低维度，我们可以用axis参数指定降维的轴

```python
#输入轴1的维数在输出形状中消失
A_sum_axis1 = A.sum(axis=1)
A_sum_axis1, A_sum_axis1.shape
```

```python
A.sum(axis=[0, 1])  # 结果和A.sum()相同
```

+ 2.调用求平均值的函数也可以指定轴降低维度


+ 3.非降维求和
  + 有时在调用函数来计算总和或均值时保持轴数不变会很有用。

```python
sum_A = A.sum(axis=1, keepdims=True)
sum_A
```
  +由于sum_A在对每行进行求和后仍保持两个轴，我们可以通过广播将A除以sum_A。
```python
A / sum_A
```

### 1.6 点积
+ 给定两个向量，它们的点积是相同位置的按元素乘积的和：

```python
y = torch.ones(4, dtype = torch.float32)
x, y, torch.dot(x, y)
```

### 1.7 矩阵-向量积

![image](https://user-images.githubusercontent.com/78517435/226540900-f385a91b-eec0-4107-8a0c-710a17c7bc79.png)

```python
#使用mv函数表示矩阵-向量积
A.shape, x.shape, torch.mv(A, x)
```

### 1.8 矩阵-矩阵乘法

![image](https://user-images.githubusercontent.com/78517435/226541224-163d4ae0-2b40-40c3-967c-34102122e857.png)

+ 将矩阵-矩阵乘法可以简单地看成执行m次矩阵-向量积，将结果拼接在一起，形成一个nxm的矩阵




### 1.9 范数
+ 向量范数是将向量映射到标量的函数f
  + 第一个性质是如果我们按常数因子缩放向量的所有元素， 其范数也会按相同常数因子的绝对值缩放：
  + 第二个性质是熟悉的三角不等式
  + 第三个性质简单地说范数必须是非负的

+ L2范数我们常常省略下标
+ 深度学习中更经常地使用范数的平方，也会经常遇到范数，它表示为向量元素的绝对值之和
+ 与L2范数相比，L1范数受异常值的影响较小

![image](https://user-images.githubusercontent.com/78517435/226540455-c3db52f9-ca07-4061-a5fc-26a8cdf7bfd3.png)


## 2.微积分
+ 在深度学习中，我们训练模型，并不断更新它们，使它们在数据越来越多的时候表现越来越好，这也就意味着最小化一个损失函数
+ 我们将拟合模型分解为两个关键问题：
  + 优化：用模型拟合观测数据的过程
  + 泛化：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。


+ 导数与微分
  + 在深度学习中，我们通常选择对于模型参数可微的损失函数。 简而言之，对于每个参数， 如果我们把这个参数增加或减少一个无穷小的量，可以知道损失会以多快的速度增加或减少。
  + 求常见函数的微分有下列一些法则
    + 常数相乘法则
    + 加法法则
    + 乘法法则
    + 除法法则

+ 偏导数
  + 将微分的思想推广到多元函数上，计算y关于第i个参数的Xi的偏导数如下：
  + ![image](https://user-images.githubusercontent.com/78517435/226559916-336049f9-6ab8-4238-b5a2-d3f3a4fb915f.png)

+ 梯度
  + 我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的梯度（gradient）向量
![image](https://user-images.githubusercontent.com/78517435/226560506-6fa9f4c6-34f2-43a9-9890-b1db5595f1ac.png)


+ 链式法则
  + 上面方法可能很难找到梯度。 这是因为在深度学习中，多元函数通常是复合的
  + 幸运的是，链式法则可以被用来微分复合函数。
  + 链式法则如下：
  + ![image](https://user-images.githubusercontent.com/78517435/226560753-9cc1c630-7d76-42fb-9a16-4e69ff1e481b.png)


## 3.概率

### 3.1 联合概率
+ 对于***任意***值a和b，联合概率可以回答：A = a和B = b同时满足的概率是多少
+ 因为A = a和B = b同时发生，所以A = a和B = b同时发生的概率不大于它们单独发生的概率


### 3.2 条件概率
+ 条件概率是B = b发生的概率，前提是A = a已经发生。













